\chapter{TCN Model Architecture}\label{chap:model}

Designing an IDS model for an SDN controller is, in many ways, an exercise in constraint satisfaction. The model has to be accurate enough to catch real attacks, fast enough to keep up with the data plane, and small enough to live alongside the controller application without starving it of memory. This chapter lays out the architecture of the TCN model---named \texttt{TCN\_InSDN}---that we use for the detection component of the TCN-HMAC framework. We walk through the residual-block design, the dilation schedule, the classification head, hyper-parameter choices, the training configuration, and finally a complexity analysis showing that the whole model fits in roughly 612~KB.

%% ============================================================
\section{Model Overview}\label{sec:model_overview}
%% ============================================================

At a high level, \texttt{TCN\_InSDN} is straightforward: six dilated causal one-dimensional residual blocks followed by a two-layer dense classification head. What makes the design interesting is the tension it resolves among accuracy, speed, and size.

The model receives input tensors of shape $(N, 24, 1)$, where $N$ is the batch size, 24 is the sequence length (corresponding to the 24 PCA-transformed features), and 1 is the number of input channels. The output is a single sigmoid-activated value representing the probability that the input flow is an attack. The complete model architecture is illustrated in Table~\ref{tab:model_arch}.

\begin{table}[ht]
\centering
\caption{TCN\_InSDN Model Architecture Summary}
\begin{tabular}{@{}clccr@{}}
\toprule
\textbf{Layer} & \textbf{Type} & \textbf{Dilation} & \textbf{Output Shape} & \textbf{Parameters} \\
\midrule
Input & --- & --- & $(N, 24, 1)$ & 0 \\
Block 1 & Residual Block & $d=1$ & $(N, 24, 64)$ & 1,536 \\
Block 2 & Residual Block & $d=2$ & $(N, 24, 64)$ & 25,216 \\
Block 3 & Residual Block & $d=4$ & $(N, 24, 64)$ & 25,216 \\
Block 4 & Residual Block & $d=8$ & $(N, 24, 64)$ & 25,216 \\
Block 5 & Residual Block & $d=16$ & $(N, 24, 64)$ & 25,216 \\
Block 6 & Residual Block & $d=32$ & $(N, 24, 64)$ & 25,216 \\
GAP & GlobalAvgPooling1D & --- & $(N, 64)$ & 0 \\
Dense 1 & Dense + BN + ReLU & --- & $(N, 128)$ & 8,576 \\
Dropout 1 & Dropout(0.2) & --- & $(N, 128)$ & 0 \\
Dense 2 & Dense + BN + ReLU & --- & $(N, 64)$ & 8,448 \\
Dropout 2 & Dropout(0.2) & --- & $(N, 64)$ & 0 \\
Output & Dense(1, sigmoid) & --- & $(N, 1)$ & 65 \\
\midrule
\multicolumn{4}{@{}l}{\textbf{Total Parameters}} & \textbf{156,737} \\
\multicolumn{4}{@{}l}{\textbf{Trainable Parameters}} & \textbf{154,817} \\
\multicolumn{4}{@{}l}{\textbf{Non-Trainable Parameters (BN)}} & \textbf{1,920} \\
\multicolumn{4}{@{}l}{\textbf{Model Size}} & \textbf{$\sim$612 KB} \\
\bottomrule
\end{tabular}
\label{tab:model_arch}
\end{table}

That is only 156,737 parameters---about 612~KB on disk. To put this in perspective, a comparable LSTM would need two to three times as many parameters, and a Transformer-based model of similar capacity would balloon to five to ten times the size. The compactness comes from weight sharing: convolutional filters are reused across every temporal position.

%% ============================================================
\section{Residual Block Design}\label{sec:residual_block}
%% ============================================================

The residual block is where most of the learning happens. Each block runs the input through a pair of dilated causal convolutions---with batch normalisation, ReLU, and spatial dropout sandwiched in---and then adds the result back to the original input via a residual shortcut.

\subsection{Block Internal Structure}\label{subsec:block_structure}

Each residual block consists of two identical sub-layers, each comprising:

\begin{enumerate}
    \item \textbf{Conv1D (causal, dilated):} A one-dimensional convolution with causal padding, dilation rate $d$, 64 filters, and kernel size 3. The causal padding ensures that the output at time step $t$ depends only on inputs at time steps $\leq t$. The number of filters (64) determines the channel dimensionality of the block's output.
    
    \item \textbf{BatchNormalization:} Normalizes the convolutional output to have zero mean and unit variance across the batch dimension, stabilizing training dynamics and enabling higher learning rates.
    
    \item \textbf{ReLU Activation:} The Rectified Linear Unit activation function $\text{ReLU}(x) = \max(0, x)$ introduces nonlinearity, enabling the network to learn complex, nonlinear decision boundaries. ReLU was chosen over alternatives (sigmoid, tanh, LeakyReLU) due to its computational simplicity, effectiveness in mitigating the vanishing gradient problem, and widespread empirical success in deep convolutional networks.
    
    \item \textbf{SpatialDropout1D(0.2):} Drops entire feature map channels with probability 0.2 during training. As discussed in Section~\ref{subsec:bg_dropout}, spatial dropout is more appropriate than element-wise dropout for one-dimensional convolutional networks because it preserves the temporal coherence of retained channels.
\end{enumerate}

The two sub-layers are applied sequentially, with the first sub-layer processing the input and producing an intermediate representation, and the second sub-layer further refining this representation to produce the block's transformed output $\mathcal{F}(\mathbf{x})$.

\subsection{Residual Skip Connection}\label{subsec:skip_connection}

The residual skip connection adds the block's input $\mathbf{x}$ to the transformed output $\mathcal{F}(\mathbf{x})$:

\begin{equation}
    \text{output} = \mathcal{F}(\mathbf{x}) + \text{shortcut}(\mathbf{x})
\end{equation}

When the input and output channel dimensions match (i.e., both are 64 channels), the shortcut is a direct identity connection: $\text{shortcut}(\mathbf{x}) = \mathbf{x}$. When the dimensions differ---as in Block 1, where the input has 1 channel and the output has 64 channels---a $1 \times 1$ convolution is applied to the shortcut path to project the input to the correct dimensionality:

\begin{equation}
    \text{shortcut}(\mathbf{x}) = W_{proj} * \mathbf{x}
\end{equation}

\noindent where $W_{proj}$ is a $1 \times 1$ convolutional kernel with 64 output channels. This projection adds $1 \times 64 + 64 = 128$ parameters (kernel weights + bias) per channel-mismatched block.

The residual connection provides the gradient with a direct path from the loss function to the early layers, mitigating the vanishing gradient problem and enabling the training of deeper networks. It also allows each block to learn an incremental refinement (residual function) rather than a complete transformation, which is generally easier to optimize.

\subsection{Block Internal Data Flow}\label{subsec:block_dataflow}

The complete data flow through a single residual block with dilation rate $d$ is formalized as:

\begin{align}
    \mathbf{h}_1 &= \text{SpatialDropout}(\text{ReLU}(\text{BN}(\text{Conv1D}_{d,64,3}(\mathbf{x})))) \\
    \mathbf{h}_2 &= \text{SpatialDropout}(\text{ReLU}(\text{BN}(\text{Conv1D}_{d,64,3}(\mathbf{h}_1)))) \\
    \mathcal{F}(\mathbf{x}) &= \mathbf{h}_2 \\
    \text{output} &= \mathcal{F}(\mathbf{x}) + \text{shortcut}(\mathbf{x})
\end{align}

\noindent where $\text{Conv1D}_{d,64,3}$ denotes a causal one-dimensional convolution with dilation rate $d$, 64 filters, and kernel size 3.

%% ============================================================
\section{Dilation Schedule}\label{sec:dilation_schedule}
%% ============================================================

The six blocks use dilation rates that double at each stage---1, 2, 4, 8, 16, 32---following the standard geometric progression $d_l = 2^{l-1}$:

\begin{table}[ht]
\centering
\caption{Dilation Schedule and Receptive Field Growth}
\begin{tabular}{@{}cccr@{}}
\toprule
\textbf{Block} & \textbf{Dilation Rate} & \textbf{Block Receptive Field} & \textbf{Cumulative Receptive Field} \\
\midrule
1 & 1 & $2 \times (3-1) \times 1 + 1 = 5$ & 5 \\
2 & 2 & $2 \times (3-1) \times 2 = 8$ & 13 \\
3 & 4 & $2 \times (3-1) \times 4 = 16$ & 29 \\
4 & 8 & $2 \times (3-1) \times 8 = 32$ & 61 \\
5 & 16 & $2 \times (3-1) \times 16 = 64$ & 125 \\
6 & 32 & $2 \times (3-1) \times 32 = 128$ & 253 \\
\bottomrule
\end{tabular}
\label{tab:dilation_schedule}
\end{table}

Because the cumulative receptive field (253) dwarfs the input length (24), the deepest layers can ``see'' the entire input sequence with room to spare. The extra coverage is not wasted---it cushions the boundary effects caused by zero-padding and makes the model more robust to shifts in the feature ordering.

The exponential dilation schedule was chosen over alternatives (linear dilation, repeated cycles) based on the following considerations:

\begin{itemize}
    \item \textbf{Exponential Efficiency:} The exponential growth of the receptive field enables coverage of the full input sequence with only $\lceil\log_2(T)\rceil$ blocks for kernel size 2, or fewer blocks for larger kernels. This minimizes the depth of the network while maximizing temporal coverage.
    
    \item \textbf{Multi-Scale Pattern Capture:} Each dilation rate captures patterns at a different temporal scale. Block 1 ($d=1$) captures fine-grained, adjacent-component patterns; Block 3 ($d=4$) captures medium-scale patterns spanning 4--8 components; Block 6 ($d=32$) captures global patterns spanning the entire input. This multi-scale hierarchy enables the model to detect both localized anomalies (e.g., a single feature spike) and distributed anomalies (e.g., correlated changes across many features).
    
    \item \textbf{Empirical Validation:} The exponential dilation schedule has been demonstrated to be effective in the original TCN architecture \cite{bai2018empirical} and in subsequent TCN-based intrusion detection studies \cite{Lopes2023TCN, LiLi2025TCNSE}.
\end{itemize}

%% ============================================================
\section{Classification Head}\label{sec:classification_head}
%% ============================================================

The classification head converts the temporal feature representation produced by the residual blocks into a binary classification output. It consists of three sequential stages:

\subsection{Global Average Pooling}\label{subsec:gap_model}

GlobalAveragePooling1D aggregates the temporal feature maps of shape $(N, 24, 64)$ into a fixed-length vector of shape $(N, 64)$ by computing the average activation across the 24 time steps for each of the 64 channels. This produces a compact representation that captures the overall statistical behavior of each feature channel across the entire input sequence.

\subsection{Dense Layers}\label{subsec:dense_layers}

Two dense (fully connected) layers progressively reduce the representation dimensionality while capturing nonlinear interactions between the pooled features:

\begin{itemize}
    \item \textbf{Dense Layer 1:} 128 neurons with batch normalization, ReLU activation, and dropout($p=0.2$). This layer expands the 64-dimensional pooled representation to 128 dimensions, enabling the network to model higher-order feature interactions. The batch normalization and dropout provide regularization to prevent overfitting in this high-dimensional space.
    
    \item \textbf{Dense Layer 2:} 64 neurons with batch normalization, ReLU activation, and dropout($p=0.2$). This layer compresses the representation back to 64 dimensions, distilling the learned interactions into a compact form suitable for the final classification.
\end{itemize}

\subsection{Output Layer}\label{subsec:output_layer}

The output layer is a single neuron with sigmoid activation:

\begin{equation}
    p(\text{attack} \mid \mathbf{x}) = \sigma(\mathbf{w}^T \mathbf{h} + b) = \frac{1}{1 + e^{-(\mathbf{w}^T \mathbf{h} + b)}}
\end{equation}

\noindent where $\mathbf{h}$ is the 64-dimensional output of Dense Layer 2, $\mathbf{w}$ is the weight vector, $b$ is the bias, and $\sigma(\cdot)$ is the sigmoid function. The sigmoid output is interpreted as the probability that the input flow is an attack. At inference time, a threshold of 0.5 is applied: flows with $p > 0.5$ are classified as attacks, and flows with $p \leq 0.5$ are classified as benign.

%% ============================================================
\section{Hyperparameter Selection}\label{sec:hyperparameters}
%% ============================================================

The dense layers were not chosen by guesswork. We swept over several hyperparameter axes on the validation set and settled on the configuration below.

\begin{table}[ht]
\centering
\caption{TCN Model Hyperparameters}
\begin{tabular}{@{}llp{7cm}@{}}
\toprule
\textbf{Hyperparameter} & \textbf{Value} & \textbf{Justification} \\
\midrule
Number of Residual Blocks & 6 & Provides receptive field of 253, exceeding input length 24 \\
Filters per Block & 64 & Balances representational capacity with model size \\
Kernel Size & 3 & Standard choice for TCN; captures local patterns effectively \\
Dilation Factors & [1,2,4,8,16,32] & Exponential growth ensures full input coverage \\
Spatial Dropout Rate & 0.2 & Moderate regularization; prevents overfitting without excessive information loss \\
Dense Layer 1 Neurons & 128 & Sufficient capacity for feature interaction modeling \\
Dense Layer 2 Neurons & 64 & Dimensionality reduction before output \\
Dense Dropout Rate & 0.2 & Consistent with block dropout for uniform regularization \\
\bottomrule
\end{tabular}
\label{tab:hyperparams}
\end{table}

\textbf{Number of Filters (64):} The choice of 64 filters per block balances model capacity with computational efficiency. A filter count of 32 was found to underfit the data (validation accuracy plateaued at approximately 99.5\%), while 128 filters provided marginal accuracy improvement (less than 0.1\%) at the cost of tripling the model size. The 64-filter configuration achieves near-optimal performance while keeping the model compact enough for real-time deployment.

\textbf{Kernel Size (3):} A kernel size of 3 is the standard choice in TCN architectures, providing a local receptive field that captures relationships between adjacent elements in the sequence. Larger kernel sizes (5 or 7) were evaluated but showed no significant accuracy improvement on the validation set while increasing the parameter count. The effectiveness of kernel size 3 is enhanced by the dilated convolutions, which effectively create ``virtual'' kernel sizes of $3 \cdot d$ at each dilation level.

\textbf{Dropout Rate (0.2):} The dropout rate of 0.2 (dropping 20\% of channels/neurons) represents a moderate level of regularization. This rate was determined through a sweep over values $\{0.1, 0.15, 0.2, 0.25, 0.3\}$ on the validation set. Rates below 0.15 showed signs of mild overfitting (validation loss increasing while training loss continued to decrease), while rates above 0.25 caused underfitting (lower training accuracy without corresponding improvement in validation accuracy).

%% ============================================================
\section{Training Configuration}\label{sec:training_config}
%% ============================================================

\subsection{Optimizer}\label{subsec:optimizer}

The Adam optimizer \cite{kingma2015adam} is used for training the TCN model. Adam combines the benefits of two gradient descent variants: AdaGrad (which adapts the learning rate individually for each parameter based on historical gradient magnitudes) and RMSProp (which uses exponentially weighted moving averages of squared gradients to normalize the parameter updates). The Adam update rule is:

\begin{align}
    m_t &= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
    v_t &= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \\
    \hat{m}_t &= \frac{m_t}{1 - \beta_1^t} \\
    \hat{v}_t &= \frac{v_t}{1 - \beta_2^t} \\
    \theta_t &= \theta_{t-1} - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
\end{align}

\noindent where $g_t$ is the gradient at time step $t$, $m_t$ and $v_t$ are the first and second moment estimates, $\beta_1 = 0.9$ and $\beta_2 = 0.999$ are the exponential decay rates, $\alpha = 10^{-3}$ is the initial learning rate, and $\epsilon = 10^{-7}$ is a numerical stability constant.

Adam was chosen over alternatives (SGD with momentum, AdamW \cite{loshchilov2019decoupled}, RMSProp) for several reasons: (1) it converges faster than vanilla SGD on the InSDN dataset, reaching near-optimal performance within 15--20 epochs compared to 50+ epochs for SGD; (2) its adaptive learning rates handle the heterogeneous scale of PCA-transformed features effectively; and (3) it has been demonstrated to perform well on convolutional architectures for classification tasks.

\subsection{Loss Function}\label{subsec:loss_function}

Binary cross-entropy loss is used as the training objective:

\begin{equation}
    \mathcal{L} = -\frac{1}{N}\sum_{i=1}^{N} w_{y_i} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
\end{equation}

\noindent where $y_i \in \{0, 1\}$ is the true label, $\hat{y}_i \in (0, 1)$ is the predicted probability, $w_{y_i}$ is the class weight for the true label, and $N$ is the batch size. The class weights $w_0 = 1.4258$ and $w_1 = 0.7700$ are applied to compensate for the class imbalance, as described in Section~\ref{sec:class_weighting}.

Binary cross-entropy is the standard loss function for binary classification with sigmoid output and is well-suited for probabilistic classification where the output represents a calibrated probability estimate \cite{zhang2018generalized}.

\subsection{Batch Size}\label{subsec:batch_size}

A batch size of 2,048 is used for training. This relatively large batch size is chosen for several reasons:

\begin{itemize}
    \item \textbf{GPU Utilization:} Large batch sizes maximize the utilization of GPU parallel processing capabilities, reducing per-sample computation time. On the NVIDIA T4 GPU used for training, a batch size of 2,048 achieves near-complete memory utilization without exceeding the available 16~GB of GPU memory.
    
    \item \textbf{Gradient Stability:} Larger batches produce more stable gradient estimates, reducing the noise in parameter updates and enabling more consistent convergence. This is particularly important for batch normalization, which performs best with sufficiently large batch sizes to produce accurate mean and variance estimates.
    
    \item \textbf{Training Speed:} With 127,981 training samples and a batch size of 2,048, each epoch consists of approximately 63 iterations, enabling rapid epoch completion and efficient use of the early stopping patience.
\end{itemize}

\subsection{Training Epochs and Early Stopping}\label{subsec:epochs}

The maximum number of training epochs is set to 100, but training is governed by an early stopping callback that monitors the validation AUC:

\begin{itemize}
    \item \textbf{Monitor:} Validation AUC (\texttt{val\_auc})
    \item \textbf{Patience:} 15 epochs
    \item \textbf{Mode:} Maximize
    \item \textbf{Restore Best Weights:} True
\end{itemize}

If the validation AUC does not improve for 15 consecutive epochs, training is terminated and the model weights from the epoch with the highest validation AUC are restored. This mechanism prevents overfitting by stopping training before the model begins to memorize noise in the training data. In practice, training converged at approximately 30 epochs, well before the 100-epoch maximum.

\subsection{Learning Rate Scheduling}\label{subsec:lr_schedule}

A ReduceLROnPlateau learning rate scheduler is employed to adaptively reduce the learning rate when training plateaus:

\begin{itemize}
    \item \textbf{Monitor:} Validation loss (\texttt{val\_loss})
    \item \textbf{Factor:} 0.5 (halve the learning rate)
    \item \textbf{Patience:} 7 epochs
    \item \textbf{Minimum Learning Rate:} $1 \times 10^{-6}$
\end{itemize}

This scheduler provides fine-grained control over the learning dynamics: the model begins training with a relatively high learning rate ($10^{-3}$) for rapid initial convergence, and the rate is progressively reduced as the model approaches its optimal performance, enabling finer weight adjustments in the later stages of training. The multiplicative factor of 0.5 allows the learning rate to decrease from $10^{-3}$ to the minimum $10^{-6}$ through at most 10 reduction steps, providing a smooth annealing schedule \cite{smith2019super}.

\subsection{Model Checkpointing}\label{subsec:checkpointing}

A model checkpoint callback saves the model weights at each epoch where the validation AUC improves:

\begin{itemize}
    \item \textbf{Monitor:} Validation AUC (\texttt{val\_auc})
    \item \textbf{Mode:} Maximize
    \item \textbf{Save Best Only:} True
    \item \textbf{Save Path:} \texttt{best\_tcn\_insdn.keras}
\end{itemize}

This ensures that the final model used for evaluation and deployment is the one that achieved the highest validation AUC, even if subsequent epochs showed degraded performance due to overfitting.

\subsection{Training Metrics}\label{subsec:training_metrics}

The following metrics are monitored during training and logged to a CSV file (\texttt{training\_log.csv}) for post-training analysis:

\begin{itemize}
    \item \textbf{Accuracy:} Overall classification accuracy on training and validation sets.
    \item \textbf{AUC:} Area Under the ROC Curve on training and validation sets.
    \item \textbf{Precision:} Fraction of predicted attacks that are true attacks.
    \item \textbf{Recall:} Fraction of true attacks that are correctly detected.
\end{itemize}

\subsection{Training Configuration Summary}\label{subsec:training_summary}

Table~\ref{tab:training_config} summarizes the complete training configuration:

\begin{table}[ht]
\centering
\caption{Complete Training Configuration}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Framework & TensorFlow 2.19.0 \\
Hardware & NVIDIA T4 GPU (16 GB) \\
Optimizer & Adam ($\beta_1=0.9$, $\beta_2=0.999$) \\
Initial Learning Rate & $1 \times 10^{-3}$ \\
Loss Function & Weighted Binary Cross-Entropy \\
Batch Size & 2,048 \\
Max Epochs & 100 \\
Early Stopping & patience=15, monitor=val\_auc \\
LR Reduction & factor=0.5, patience=7, min\_lr=$10^{-6}$ \\
Class Weights & [1.4258, 0.7700] \\
Checkpoint & Save best by val\_auc \\
Convergence & $\sim$30 epochs \\
\bottomrule
\end{tabular}
\label{tab:training_config}
\end{table}

%% ============================================================
\section{Model Complexity Analysis}\label{sec:complexity}
%% ============================================================

\subsection{Parameter Count Breakdown}\label{subsec:param_count}

Table~\ref{tab:param_breakdown} provides a detailed breakdown of the parameter count for each component of the TCN model:

\begin{table}[ht]
\centering
\caption{Parameter Count Breakdown by Component}
\begin{tabular}{@{}lrr@{}}
\toprule
\textbf{Component} & \textbf{Trainable} & \textbf{Non-Trainable} \\
\midrule
Block 1 (Conv1D $\times$ 2 + BN $\times$ 2 + 1$\times$1 proj) & 1,472 & 64 \\
Block 2 (Conv1D $\times$ 2 + BN $\times$ 2) & 24,960 & 256 \\
Block 3 (Conv1D $\times$ 2 + BN $\times$ 2) & 24,960 & 256 \\
Block 4 (Conv1D $\times$ 2 + BN $\times$ 2) & 24,960 & 256 \\
Block 5 (Conv1D $\times$ 2 + BN $\times$ 2) & 24,960 & 256 \\
Block 6 (Conv1D $\times$ 2 + BN $\times$ 2) & 24,960 & 256 \\
Dense 1 (128 units + BN) & 8,320 & 256 \\
Dense 2 (64 units + BN) & 8,320 & 128 \\
Output (1 unit) & 65 & 0 \\
\midrule
\textbf{Total} & \textbf{154,817} & \textbf{1,920} \\
\bottomrule
\end{tabular}
\label{tab:param_breakdown}
\end{table}

The 1,920 non-trainable parameters correspond to the running mean and variance statistics maintained by the batch normalization layers. These statistics are updated during training using exponential moving averages but are not updated by gradient descent.

\subsection{Computational Complexity}\label{subsec:computational_complexity}

The computational complexity of a single forward pass through the TCN model can be analyzed in terms of floating-point operations (FLOPs):

\textbf{Convolutional Layers:} For a Conv1D layer with $C_{in}$ input channels, $C_{out}$ output channels, kernel size $K$, and input length $T$, the number of FLOPs is approximately $2 \cdot T \cdot C_{in} \cdot C_{out} \cdot K$. For each residual block (two Conv1D layers with $K=3$, $C_{in}=C_{out}=64$, $T=24$), the convolutional FLOP count is $2 \times 2 \times 24 \times 64 \times 64 \times 3 = 1{,}179{,}648$ FLOPs per block. With 6 blocks, the total convolutional FLOPs are approximately 7.08M.

\textbf{Dense Layers:} The dense layers contribute $2 \times (64 \times 128 + 128 \times 64 + 64 \times 1) \approx 33$K FLOPs.

\textbf{Total:} The total computational cost per inference is approximately 7.1M FLOPs, which is extremely lightweight by deep learning standards. For reference, a standard ResNet-18 image classification model requires approximately 1.8G FLOPs per inference---250$\times$ more than the TCN\_InSDN model. This low computational cost enables real-time inference at thousands of flows per second on modern hardware.

\subsection{Memory Footprint}\label{subsec:memory}

The model's memory footprint during inference consists of:

\begin{itemize}
    \item \textbf{Model Parameters:} 156,737 parameters $\times$ 4 bytes (float32) $\approx$ 612~KB
    \item \textbf{Activations (per sample):} Peak activation memory occurs at the residual blocks, where the feature tensor has shape $(24, 64)$, consuming $24 \times 64 \times 4 = 6.1$~KB per sample.
    \item \textbf{Total Inference Memory:} For a single sample, the total inference memory is approximately 620~KB, which is well within the capacity of any modern computing platform.
\end{itemize}

This compact memory footprint enables deployment on resource-constrained SDN controller platforms without competing for memory with the controller application's own data structures and processing requirements.

%% ============================================================
\section{Weight Initialization}\label{sec:weight_init}
%% ============================================================

Proper weight initialization is critical for effective training of deep neural networks \cite{glorot2010understanding}. The TCN model uses the following initialization schemes:

\begin{itemize}
    \item \textbf{Convolutional Layers:} Glorot (Xavier) uniform initialization, which draws weights from a uniform distribution $\mathcal{U}(-\sqrt{6/(f_{in}+f_{out})}, \sqrt{6/(f_{in}+f_{out})})$, where $f_{in}$ and $f_{out}$ are the fan-in and fan-out of the convolutional kernel. This initialization ensures that the variance of activations is preserved across layers, preventing both vanishing and exploding activations at initialization.
    
    \item \textbf{Dense Layers:} Glorot uniform initialization, consistent with the convolutional layers.
    
    \item \textbf{Batch Normalization:} The scale parameter $\gamma$ is initialized to 1 and the shift parameter $\beta$ is initialized to 0, which makes the initial batch normalization an identity transformation (after mean and variance normalization). The running mean is initialized to 0 and the running variance to 1.
    
    \item \textbf{Bias Terms:} All bias terms are initialized to 0.
\end{itemize}

%% ============================================================
\section{Chapter Summary}\label{sec:model_summary}
%% ============================================================

This chapter has walked through every layer of the \texttt{TCN\_InSDN} model, from the first dilated convolution to the final sigmoid output. The key takeaway is that the architecture is deliberately compact: 156,737 parameters, 612~KB, roughly 7.1M FLOPs per inference, and convergence in about 30 epochs with Adam plus a cosine-style learning-rate anneal starting at $10^{-3}$. These numbers translate directly into the real-time, low-overhead deployment profile that an SDN controller needs. The experiments that put this architecture to the test come next, in Chapter~\ref{chap:results}.

\endinput
